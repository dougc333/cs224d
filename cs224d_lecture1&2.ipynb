{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>NLP </h6>\n",
    "Sourece: youtube cs224d Richard Socher \n",
    "<p>\n",
    "Trying to get the computer to understand the language. Examples of NLP applications: generate a bot creating tweets like Trump. \n",
    "A traditional hierarchy of tasks which have to be completed defines a NLP pipeline\n",
    "\n",
    "</p>\n",
    "<img src=\"socher3.png\">\n",
    "<p>\n",
    "\n",
    "</p>\n",
    "<img src=\"socher2.png\">\n",
    "<p>\n",
    "CS224d focuses on syntax analysis and semantic interpretation.\n",
    "Might skip levels to get to the final applicaiton. \n",
    "\n",
    "2 representations. First is a language model based on probabilities. Second is one based on trees and sentence\n",
    "structure. Most implementations use mixtures of the 2. \n",
    "</p>\n",
    "<img src='lmchainrule.png'>\n",
    "<a href='https://nlp.stanford.edu/sentiment/treebank.html' target='_blank' >sentiment treebank</a>\n",
    "<img src=\"socher1.png\">\n",
    "<p>\n",
    "</p>\n",
    "<p>\n",
    "Problems: did not do anti-disambiguation properly. When Anne Hathaway positive reviews came out Berkshire Hathaway\n",
    "    stock went up. \n",
    "</p>\n",
    "<img src='socherberk.png'>\n",
    "<img src='socher4.png'>\n",
    "<p>\n",
    "Steps to nlp; representation built up from basic units. Start with DL for speech. THe applicaiton was to take in raw \n",
    "waveforms of sounds and predict phoenomes and words as the output. The word error rate dropped significantly using DL.  \n",
    "</p>\n",
    "<img src='socher5before.png'>\n",
    "Common theme is to represent \n",
    "units (words, phoenomes) as vectors. Start with phoenomes->morphemes (subunits of words)\n",
    "<img src='socher5.png'>\n",
    "Showing the combination of 2 morphemes to the word unfortunate. Morphenes+DL / Train the prediciton of morpheme sequences into words\n",
    "using RNNs. \n",
    "<img src='socher6.png'>\n",
    "<p>\n",
    "</p>\n",
    "Syntax analysis, classifying phrases, NP, VP. DL allows us to form similarity measures so the NP the dog is similar to the cat\n",
    "and this can be used to generate similar sentences by substitution of similar phrases. \n",
    "<img src='sochersyntax.png'>\n",
    "Does the sentence all repiles walk entail the sentence all turtles walk? \n",
    "<img src = 'socher7.png'>\n",
    "DL/Sentiment analysis is a binary classification which takes in raw phrases/sentences and does away with all the above model/data pipeline of morphology/syntax/semantics. \n",
    "DL was able to outperform all the non DL approaches. Need 10s of thousands of examples per class. Skewed distribution problem\n",
    "if 90% is positive and 10% negative then model is biased and wont perform well. techniques to deal with this. Sentiment analysis implemented \n",
    "with RNN and addition of attention mechanism. Called dynamic memory network. \n",
    "<img src='socherqa.png'>\n",
    "DL QA system outperformed. facts are stored in vectors. The model gives similarities ie former presidents clustered together\n",
    "automatically. \n",
    "<img src='sochermt1.png'>\n",
    "seq2seq input vector and output vector. Different than binary output of sentiment classifier.\n",
    "<img src='sochermt2.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>Lecture 2 Word Vectors</h6>\n",
    "The key from all the slides from the last lecture was in morphology/syntax/semantics are the representation of vectors. \n",
    "<p>\n",
    "Word vectors, how do we represent the meaning of a word in a computer? We can build lists of word associations. Hypernym example below\n",
    "We could also use the word next to the word to establish relationships. Firth 1957 idea. Use the frequence of words and which \n",
    "words are nearest. \n",
    "</p>\n",
    "Given a sample corpus create a DAG using either an adjacency matrix or adjacency list to represent word locality to each other. If we \n",
    "want to create probability distributions we can assign 1 to words immediately to right/left then assign smaller numbers as the\n",
    "words are further away. This is one way to represent words as vectors. We can see like and enjoy are similar. This is an improvement\n",
    "over a one-hot representation. This matrix is too large to be practical for real world situations. Use a lower dimensional vector vs. a sparse matrix.\n",
    "Most of our applications we have small data with 25-100dim word vectors. First approach use SVD to reduce dimensionality. Second approach is word2vec!!!\n",
    "\n",
    "<img src='socherlec2_1.png'>\n",
    "<img src='socherlec2_2.png'>\n",
    "After SVD the dimensonality reduction yields similar words which are close together. This is from ~2005. Before word2vec.\n",
    "Can use PCS/t-SNE to visualize words also. Compare/contrast these. There are additional improvements to t-SNE. This is a good baseline b/c Yelp reviews\n",
    "use a limited vocabulary since it is domain specific. There are some problems with this. When you add documents you have to\n",
    "rerun SVD. There are online SVD algos.. for incremental changes...\n",
    "<img src='socherlec2_3.png'>\n",
    "<p>As prereq to word2vec Socher recommennds: http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\n",
    "        and http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf and https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf\n",
    "</p>\n",
    "<p>\n",
    "For word2vec we are going to predict the surrounding words only. There are 2 implementations, GloVe and word2vec. These 2 methods\n",
    "capture co-occurrence statistics. Can add new documents/words easily without having to rerun the entire algorithm like in SVD.\n",
    "</p>\n",
    "The objective function or word2vec. The co-occurrence window is of length m. \n",
    "Try to predict m words to the right and m words to the left. \n",
    "<img src='socherlec2_4.png'>\n",
    "<img src='socherlec2_5.png'>\n",
    "<h6>Logistic Regression, MLE  </h6>\n",
    "<p>Added material: this is not part of the lecture</p>\n",
    "<p>From Regression analysis by example, stat202 chapter 12/Logistic Regression</p>\n",
    "<p>Logistic Regression is for modeling categorical data. Words are considered categorical. Each word is a separate\n",
    "category in the simplest example. </p>\n",
    "<p>Consider the case of 1 predictor where we want pi to be a probability between 0-1, </p>\n",
    "$\\pi = Pr(Y=1 | X=x) = \\beta_{0} + \\beta_{1}x + \\epsilon$\n",
    "<p> which is the standard approach. Trying to fit\n",
    "a linear equation to a probability distribution between 0 and 1 can't be done with a linear equation as \n",
    "for linear regression. One problem is the linear equation is unbounded. This method may be possible but the standard development\n",
    "is to consider an alternate formulation for pi. Note: a logistic/softmax is just a function which returns a number between 0 and 1.\n",
    "<p>We have not modified this to follow any physical rules or properties of a real system...As Socher states in lecture</p>\n",
    "\n",
    "</p>\n",
    "<p>The probability distribution of pi can be assumed be a logistic/softmax. Logistic for binary classification and\n",
    "softmax for multiclass classificaiton. This is the simplest probability distribution \n",
    "between 0 and 1 which scales to large amounts of data. </p>\n",
    "<p>Softmax:\n",
    "    $\\pi = Pr(Y=1 | X=x) = \\frac{exp(\\beta_{0} + \\beta_{1}x + \\epsilon)}{\\sum_{k=1}^{K} exp(\\beta_{0} + \\beta_{1}x + \\epsilon)}$\n",
    "    </p>\n",
    "\n",
    "<h6>End added material</h6>\n",
    "<p>The following is from word2vec. He didn't say\n",
    "anything about softmax before introducing this. He did say this scheme was easy to update w new words.\n",
    "Looking at objective function of word2vec. Maximing the log proability of the center word. For eacn word\n",
    "we are going to look m words to right and m words to left. We are going to maximize the log progability. How\n",
    "to represent in the simplest way the probabiltiy of the words around the center word. o is the outside word or output \n",
    "word id. c is center wrod. u and v are center and outside vectors of o and c. \n",
    "To produce a probablity distribution over a set of classes we use a softmax defined as:</p>\n",
    "$p(o|c) = \\frac{exp(u_o^Tv_c)}{\\sum_{w=1}^{W}exp(u_w^Tv_c)}$ \n",
    "<p></p>\n",
    "<p>derivative definitions for this specific case/following Socher conventiopn</p>\n",
    "$\\frac{\\partial x^Ta}{\\partial x} = a$\n",
    "<p></p>\n",
    "$\\frac{\\partial x^Ta}{\\partial a} = x$\n",
    "<p>Taking the log of both sides:</p>\n",
    "$log(p(o|c)) = log \\frac{exp(u_o^Tv_c)}{\\sum_{w=1}^{W}exp(u_w^Tv_c)}$\n",
    "<p></p>\n",
    "$= log(exp(u_o^Tv_c)) - log(\\sum_{w=1}^{W}exp(u_w^Tv_c))$\n",
    "<p>Taking the derivative</p>\n",
    "$\\frac{\\partial log(exp(u_o^Tv_c))}{\\partial v_c} - \\frac{\\partial(log(\\sum_{w=1}{W} exp(u_w^T v_c)))}{\\partial v_c} $\n",
    "<p></p>\n",
    "$\\frac{\\partial log(exp(u_o^Tv_c))}{\\partial v_c} = \\frac{\\partial u_o^T}{\\partial v_c} = u_o$\n",
    "<p></p>\n",
    "$\\frac{\\partial log(z)}{\\partial z} =\\frac{1}{z}dz $\n",
    "<p></p>\n",
    "$z =\\sum_{w=1}^{W} exp(u_w^T v_c) $\n",
    "<p></p>\n",
    "$\\frac{\\partial exp(x)}{\\partial x} = exp(x)dx$\n",
    "<p></p>\n",
    "$\\frac{\\partial (- log(\\sum_{w=1}^{W} exp(u_w^T v_c)) ) }{\\partial v_c} =$\n",
    "$-\\frac{1}{\\sum_{w=1}^{W} exp(u_w^T v_c)} \\cdot \\frac{\\partial exp(\\sum_{x=1}^{W}u_x^Tv_c)}{\\partial v_c}$\n",
    "<p></p>\n",
    "$= -\\frac{1}{\\sum_{w=1}^{W} exp(u_w^T v_c)} \\cdot \\sum_{x=1}^{W}exp(u_x^T v_c) \\cdot \\frac{\\partial(u_x^T v_c)}{\\partial v_c} $\n",
    "<p></p>\n",
    "$\\frac{\\partial log(p(o|c))}{\\partial v_c}=u_o -\\frac{1}{\\sum_{w=1}^{W} exp(u_w^T v_c)} \\cdot \\sum_{x=1}^{W}exp(u_x^T v_c) \\cdot u_x$\n",
    "<p></p>\n",
    "$=u_o - \\sum_{x=1}^{W} p(x|c) u_x$\n",
    "<p></p>\n",
    "<p>\n",
    "The problem with the above equation is there is a sliding window for all the words in the vocabulary/set of documents and is\n",
    "impractical to compute. \n",
    "<p>Futher explanation of word2vec</p>\n",
    "<a href='https://arxiv.org/pdf/1402.3722.pdf' target='_new'>word2vec explained</a>\n",
    "</p>\n",
    "<h6>Further development of softmax</h6>\n",
    "http://smerity.com/articles/2017/mixture_of_softmaxes.html\n",
    "<p></p>\n",
    "\n",
    "<img src='socherlec2_6.png'>\n",
    "<img src='socherlec2_7.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<h6>Word/Phrase similarity</h6>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pathToGoogleNewsWord2Vec = '/Users/dc/Documents/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "model = word2vec.Word2Vec.load_word2vec_format(pathToBinVectors, binary=True)\n",
    "list_phrases = ['The brown fix jumped over the lazy dog']\n",
    "wordvec_phrase = [model(x) for x in phrase[0]]\n",
    "#300, vector per word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<h6>From Lecture Word2Vec again</h6>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset('panda.n.01')\n",
    "hyper = lambda s:s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model1 = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/dc/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "-0.00430329\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from random import sample\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "#print(model1['computer'])\n",
    "phrase1 = \"the brown fox jumped over the dog\"\n",
    "phrase2 = 'the dog ate the dead fox'\n",
    "\n",
    "#cachedStopWords = stopwords.words(\"english\")\n",
    "p1_lower = phrase1.lower()\n",
    "p2_lower = phrase2.lower()\n",
    "\n",
    "p1_list = p1_lower.split() \n",
    "p2_list = p2_lower.split()\n",
    "p1_vec = [model1[x] for x in p1_list]\n",
    "p2_vec = [model1[x] for x in p2_list]\n",
    "\n",
    "#print(len(p1_vec),len(p2_vec))\n",
    "#word2vec is 1 word=>[300 entries]\n",
    "print(p1_vec[0].shape)\n",
    "p1_mean_0 = np.mean(p1_vec[0])\n",
    "print(p1_mean_0)\n",
    "p1_mean = np.mean(p1_vec)\n",
    "p2_mean = np.mean(p2_vec)\n",
    "\n",
    "p1_vec_norm = [x-p1_mean for x in p1_vec]\n",
    "p2_vec_norm = [x-p2_mean for x in p2_vec]\n",
    "\n",
    "print(p1_vec_norm[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>seq2seq</h6>\n",
    "Input/Output are vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>demo</h6>\n",
    "https://nlp.stanford.edu/sentiment/treebank.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>Data Augmentation in feature space</h6>\n",
    "https://arxiv.org/pdf/1702.05538.pdf\n",
    "<h6>Rossmann</h6>\n",
    "Link to Rossmann\n",
    "<h6>Frame Semantics Parser</h6>\n",
    "https://github.com/google/sling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>tsne</h6>\n",
    "Lion tsne: https://arxiv.org/abs/1708.04983\n",
    "large visualization: https://github.com/lferry007/LargeVis\n",
    "tsne better than pca? \n",
    "pca muffin/chihuaha: \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h6>Sentiment Analysis</h6>\n",
    "I am not sure how all the approaches contrast with each other; what the tradeoffs are. \n",
    "https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
